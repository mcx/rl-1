# DQN Trainer Configuration for CartPole-v1

defaults:

  - transform@transform0: step_counter
  - transform@transform1: reward_sum

  - env@training_env: batched_env
  - env@training_env.create_env_fn: transformed_env
  - env@training_env.create_env_fn.base_env: gym
  - transform@training_env.create_env_fn.transform: compose

  - model@models.qvalue_model: qvalue

  - network@networks.qvalue_network: mlp

  - collector@collector: sync

  - replay_buffer@replay_buffer: base
  - storage@replay_buffer.storage: lazy_tensor
  - writer@replay_buffer.writer: round_robin
  - sampler@replay_buffer.sampler: random
  - trainer@trainer: dqn
  - optimizer@optimizer: adam
  - loss@loss: dqn
  - target_net_updater@target_net_updater: hard
  - logger@logger: csv
  - _self_

# Network configurations
networks:
  qvalue_network:
    out_features: 2  # CartPole has 2 actions
    in_features: 4   # CartPole observation space is 4-dimensional
    num_cells: [120, 84]

# Model configurations
models:
  qvalue_model:
    in_keys: ["observation"]
    out_keys: ["action_value"]
    action_space: "one-hot"
    network: ${networks.qvalue_network}

transform0:
  max_steps: 500
  step_count_key: "step_count"

transform1:
  in_keys: ["reward"]
  out_keys: ["reward_sum"]

training_env:
  num_workers: 1
  create_env_fn:
    base_env:
      env_name: CartPole-v1
    transform:
      transforms:
        - ${transform0}
        - ${transform1}
    _partial_: true

# Loss configuration
loss:
  value_network: ${models.qvalue_model}
  loss_function: l2
  delay_value: true
  action_space: "one-hot"

target_net_updater:
  value_network_update_interval: 50

# Optimizer configuration
optimizer:
  lr: 2.5e-4

# Collector configuration
collector:
  create_env_fn: ${training_env}
  policy: ${models.qvalue_model}
  total_frames: 500_000
  frames_per_batch: 1000
  init_random_frames: 10_000
  _partial_: true

# Replay buffer configuration
replay_buffer:
  storage:
    max_size: 10_000
    device: cpu
    ndim: 1
  sampler:
  writer:
    compilable: false
  batch_size: 128


logger:
  exp_name: dqn_cartpole_v1

# Trainer configuration
trainer:
  collector: ${collector}
  optimizer: ${optimizer}
  replay_buffer: ${replay_buffer}
  target_net_updater: ${target_net_updater}
  loss_module: ${loss}
  value_network: ${models.qvalue_model}
  logger: ${logger}
  total_frames: ${collector.total_frames}
  frame_skip: 1
  clip_grad_norm: true
  clip_norm: 10.0
  progress_bar: true
  seed: 42
  save_trainer_interval: 10000
  log_interval: 10000
  save_trainer_file: null
  optim_steps_per_batch: 100
  eps_init: 1.0
  eps_end: 0.05
  annealing_num_steps: 250_000
  async_collection: false
